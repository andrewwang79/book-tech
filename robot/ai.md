# 智能化
## 智能三要素
* 缺一不可，从上向下依赖。

| 项 | 重要的原因 | 碳基生物的类比 |
| - | - | - |
| 智能算法 | 决策中枢，决定智能化水平 | 大脑 |
| 软件 | 提供(采集、清理、预处理)算法多感官数据融合 | 本体、感官(视觉、触觉、力觉) |
| 系统架构 | 机器人是非常复杂的系统(特别是人形具身机器人)，对系统架构要求很高。 <br/> 硬件拓扑结构决定数据流动路径(类似人体神经分布) <br/> 数据链路决定通信延迟(类似神经冲动传导速度) | 硬件拓扑+数据链路相当于神经子系统。其他还有散热子系统 |

## 目标
1. 决策：正确
1. 决策执行：高速。人类反应时间：无意识是12-50毫秒，有意识是100-300毫秒
1. 动作执行：协作，精准

## 数据
### 分类
1. 本体/：自身状态姿态和运动参数，如机械臂空间坐标、关节参数(位置，力矩)
1. 感官
    1. 视觉：摄像头
    1. 触觉：压力皮肤， “触摸识别” 物体材质
    1. 力觉：力反馈

### 采集
* 收集每个动作的数据
    * 动作越复杂、精度越精细，则需要的数据越多
    * 多数据源融合，形成对环境和自身状态的统一表征
* 数据要有不同的收集设备，如双目摄像头
* 数据量
    * 真实数据。案例：搭建模拟场景的数据工厂，用VR眼镜真人施教100台机器人，半年内采集百万条数据
    * 虚拟数据：仿真环境

## 算法训练
### 融合
1. (多模态)数据融合：双目相机的深度图，碰触压力分布图，机械臂参数
1. 特征融合：复合特征向量含所有数据类目
1. 决策融合：多任务决策（如 “是否抓取”，“抓取路径 + 抓取位置 + 抓取力度”）

### 感知模型
* 对外界的感知，类似人眼睛和手感受世界。
* 模型输入数据不止视觉【将摄像头视频与毫米波雷达点云融合，实现动态障碍物预测】
* 感知物体和事件：如血管、动脉出血(出血点、出血流速)
* 训练方法：监督学习
* 训练
    1. 基于标识数据获取特征融合
    1. 感知模型训练

### 决策模型
#### 训练方法
1. 模仿学习 + 强化学习(从试错到本质规律的探索)
1. 模仿学习：学习专家的
1. 强化学习的Reward尽量是业务奖励，而不是操作奖励。以下用手术机器人止血任务举例：
    * 操作奖励「手指关节角度每偏差 < 5° 得 1 分」：机器人会过度拟合特定抓取姿势，面对新物体时因角度微差失败（类似学生死记硬背公式，不懂灵活应用）。
    * 业务奖励「30 秒内止血成功 + 100 分，组织损伤 > 5% 扣 50 分」：机器人会自主探索最优止血路径（如先压迫再缝合），而非机械模仿示教动作。

#### 训练
1. 特征融合
1. 决策模型训练：基于特征融合训练决策模型
    * 各种场景下的处理方案(事件对应的任务)
    * 每个任务方法和策略

## 日常使用全流程
| 步骤 | 内容 | 手术示例（动脉止血） |
| - | - | - |
| 数据采集和特征融合 | 多模态数据同步采集与跨模态特征提取 | 内窥镜视频，血压 |
| 感知 | 环境语义理解与目标识别 | 动脉，动脉出血 |
| 决策 | 任务规划与动作参数生成 | 动脉出血了：生成任务“电凝止血”，机械臂运行路径是X，电凝策略是时间（3-5 秒）与压力（0.3-0.5N） |
| 执行 | 物理动作精准控制与闭环反馈 | 电凝钳执行 |

## 工程化
* 时空对齐：通过时间戳同步多传感器数据（延迟 < 1ms）
* 影子模式：收集数据模拟算法执行，模拟结果比对人的决策结果、结果不匹配的上传分析